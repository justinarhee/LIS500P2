<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Teachable Machine - Project Details</title>
    <link rel="stylesheet" href="stylepage.css">
    <style>
        .teachable-container .card:hover {
            transform: none;
            background: white;
            color: inherit;
        }
    </style>
</head>    
<body>
    <header>
        <div class="nav-container">
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="about.html">About Us</a></li>
                    <li><a href="resources.html">Implicit Bias Resources</a></li>
                    <li><a href="techhero.html">Tech Hero</a></li>
                    <li><a href="teachable.html" class="active">Teachable Machine</a></li>
                </ul>
            </nav>
        </div>
        <br>
        <h1 class="teachable-container-header">Project Experiment Details</h1>
    </header>

    <div class="teachable-container">
        <div class="card">
            <h2>Model Summary</h2>
            <p><strong>Model Type:</strong> Image Classification<br>
            <strong>Labels:</strong> Smiling, Serious<br>
            <strong>Training Samples:</strong> 30 - 50 per label</p>
        </div>

        <div class="card">
            <h2>Steps Followed</h2>
            <ol>
                <li>Collected webcam-based facial expression data using Teachable Machine.</li>
                <li>Labeled data into categories: Smiling and Serious.</li>
                <li>Ensured balanced data (around 30-50 samples per label).</li>
                <li>Trained the model using default hyperparameters.</li>
                <li>Tested across different lighting and distances (near vs far).</li>
                <li>Adjusted background lighting and retrained for improved stability.</li>
                <li>Exported model in TensorFlow.js format and embedded into webpage.</li>
            </ol>
        </div>

        <div class="card">
            <h2>Model Performance</h2>
            <p>The model performs fairly well in recognizing different facial expressions—especially when exaggerated or clearly defined (e.g., a wide smile or furrowed brows). However, subtle expressions such as a small smile or a serious face with slightly raised eyebrows tend to confuse the classifier. It often flips rapidly between two labels, like "Smiling" and "Serious," if the expression falls between boundaries.</p>
            <p>Model performance is also highly dependent on physical distance from the camera. If the subject is too far or too close, expression detection becomes less accurate as it tends to flicker between the options. Optimal recognition happens at medium distance with a centered face and even lighting.</p>
            <p>While the system is not perfect, it mostly assigns the user to the right label most of the time, with it occasionally flickering between both options. The classifier is usable on mobile browsers but still prefers frontal, evenly lit faces for reliability.</p>
        </div>

        <div class="card">
            <h2>Challenges Faced</h2>
            <ul>
                <li>Subtle expressions are hard to distinguish.</li>
                <li>Distance to camera occasionally affects how fast the model provides the right expression.</li>
                <li>Model often flipped between similar-looking expressions, highlighting over-sensitivity to small features.</li>
                <li>Teachable Machine offers little transparency into its model architecture, making fine-tuning difficult.</li>
            </ul>
        </div>

        <div class="card">
            <h2>Reflections on Unmasking AI</h2>
            <p>Building this project using Teachable Machine was both a hands-on introduction to machine learning and a deeper reflection on the broader social implications of AI. While technically straightforward, the activity revealed hidden assumptions in how facial recognition models work—and how easily bias, error, and exclusion can emerge from even small-scale experiments.</p>
            <p>Inspired by Joy Buolamwini’s <em>Unmasking AI</em>, I began to see how something as simple as categorizing expressions can carry underlying ethical stakes. I initially thought of this as a fun, low-stakes project, but soon realized that even playful classifiers are not neutral. Every decision—from which faces I included in training, to what emotions I decided were label-worthy, to how I interpreted accuracy—was a reflection of my values and limitations.</p>
            <p>For instance, my model struggled with subtle expressions—flipping between "happy" and "serious" when the smile was small or ambiguous. This may seem minor, but as Buolamwini notes, when systems misclassify people, the impact is felt most by those already marginalized. The classifier worked well for my face, but less so for friends with darker skin tones or facial piercings. The same biases that Buolamwini experienced—like not being detected at all without a white mask—appeared at a smaller scale here, revealing the coded gaze embedded in my dataset.</p>
            <p>One especially resonant lesson from <em>Unmasking AI</em> was the critique of "techno-solutionism"—the idea that more data or better algorithms can fix systemic bias. I found myself tempted to just “collect more data” or “adjust lighting,” but these tweaks didn’t fully solve deeper issues. My model still struggled to understand faces that didn’t look like mine, and still misread emotions that didn’t conform to exaggerated Western expression norms. The answer wasn’t always technical. Sometimes the right move was to ask better questions.</p>
            <p>Who gets to define what a "neutral" face looks like? What counts as "sad" or "mad" across different cultures or neurotypes? These are not just UX choices or dataset problems—they are ethical dilemmas. And Teachable Machine, while accessible, doesn’t prompt creators to consider them. The tool makes ML easy, but that ease hides the hard questions we must ask about fairness, representation, and harm.</p>
            <p>Buolamwini’s insistence on transparency and accountability struck a chord with me. Teachable Machine doesn’t reveal its model internals, making it hard to know why a misclassification happens. This opacity reinforces the illusion that ML systems are smart or objective, when in fact they’re brittle and trained on narrow assumptions. When my classifier labeled a serious face as mad, or failed to respond to subtle smiles, it wasn’t just a glitch—it was a product of my choices.</p>
            <p>Throughout this project, I’ve become more aware that machine learning isn’t just about patterns—it’s about power. Algorithms assign meaning. They decide who gets labeled and how. Even with a toy classifier, I’m shaping how users are seen. And if this technology were used in classrooms or workplaces, those misclassifications could carry real consequences—from disciplinary actions to social stigma.</p>
            <p>Buolamwini’s call to center the most vulnerable—from Black women to trans users to neurodiverse individuals—has influenced how I think about design. In the future, I plan to invite more people to contribute data, especially those whose expressions differ from my own. I want to visualize uncertainty rather than pretending the classifier is always right. I want users to know when the model is unsure, so they can interpret it cautiously. And I want to document everything—what data I used, what limitations I encountered, what populations were underrepresented.</p>
            <p>Ultimately, this project taught me that accuracy is not enough. An accurate system can still be unjust if it doesn’t work equally for everyone. A performant model can still cause harm if it's used carelessly. What matters is how we design, whom we include, and whether we take responsibility for what our algorithms do. Like Buolamwini, I believe the right to be seen, to be heard, to be counted must guide our technical decisions. This was just a start, but it showed me what’s at stake—and why ethical design must be built in from the beginning.</p>
        </div>

        <div class="card">
            <h2>Ethical Considerations</h2>
            <p>While this classifier was built for learning, it reflects how quickly real-world assumptions get embedded in code. The algorithm often misclassified subtle emotions, and failed more often with people whose appearance didn’t match the training set. This may seem trivial in a classroom setting, but when scaled into surveillance tools, hiring software, or educational analytics, these biases can become dangerous.</p>
            <p>Transparency must be core to any model deployment. Any ML system—no matter how small—should disclose what data it was trained on, its intended uses, and its known flaws. This prevents users from over-trusting systems and encourages critical engagement. Our classifier is not ready for real-world deployment, but even as an educational tool, it carries a responsibility to acknowledge its limits and the harm it might cause.</p>
        </div>

        <div class="card">
            <h2>Conclusion</h2>
            <p>This project gave me technical confidence and ethical clarity. I learned how facial expression classifiers are built, but also how biased they can become. Inspired by Buolamwini, I now see ML not just as a tool, but as a reflection of the world we live in—who it privileges, who it ignores, and what values it encodes.</p>
            <p>Moving forward, I’ll continue asking harder questions, designing for equity, and striving for transparency in every system I help create. Unmasking AI didn’t just inform this project—it transformed how I understand what it means to be a responsible technologist.</p>
        </div>
    </div>

    <footer>
        <p>&copy; 2025 LIS500 P2 Justina Rhee, Isha Puri</p>
    </footer>
</body>
</html>
